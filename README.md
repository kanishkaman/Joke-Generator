# Joke Generation using Decoder Models & GPT-2 Fine-tuning

This project explores two approaches to generating jokes:  
1. **Training a Transformer-based decoder model from scratch**  
2. **Fine-tuning a pre-trained GPT-2 model**  

The objective is to compare the performance of these models in joke generation, analyzing their coherence, humour quality, and overall effectiveness. (and to show the superiority of GPT, over something I built)

## ðŸ“Œ Features
- **Custom Transformer Decoder Model**: Built from scratch to generate jokes.
- **Fine-Tuned GPT-2 Model**: Adapted for joke generation using a curated dataset.
- **Dataset Preprocessing & Cleaning**: Merged and filtered joke datasets for high-quality training data.
- **Hyperparameter Tuning**: Conducted multiple experiments with different model configurations.
- **Comparative Analysis**: Evaluated both models on joke coherence and humor generation.

## ðŸ“‚ Repo Structure
```plaintext
Joke-Generation/
â”œâ”€â”€ Problem_Statement.ipynb     # Problem statement and initial tasks
â”œâ”€â”€ Solution.ipynb              # Implementation of decoder model and GPT-2 fine-tuning
â”œâ”€â”€ Report3.pdf                 # Detailed report analyzing the models
â”œâ”€â”€ LICENSE                     # MIT License
â””â”€â”€ README.md                   # This documentation

