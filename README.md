# Joke-Generator
This project explores joke generation using a Transformer-based decoder model trained from scratch, and then using a fine-tuned GPT-2 model for the same. It compares both approaches in terms of coherence, humour quality, and model performance.
